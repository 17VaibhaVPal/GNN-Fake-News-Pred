{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13973691,"sourceType":"datasetVersion","datasetId":8908649}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:28.229810Z","iopub.execute_input":"2025-12-03T15:43:28.230025Z","iopub.status.idle":"2025-12-03T15:43:30.398683Z","shell.execute_reply.started":"2025-12-03T15:43:28.230008Z","shell.execute_reply":"2025-12-03T15:43:30.397986Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fake-news-dataset/train.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell A — locate train.tsv automatically\nimport os\n\ncandidates = []\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for f in files:\n        if f.lower().endswith((\"train.tsv\",\"train.csv\",\"train.txt\")):\n            candidates.append(os.path.join(root, f))\n\nprint(\"Found candidate files:\")\nfor p in candidates:\n    print(\" \", p)\n\n# if none found, show top-level datasets\nif not candidates:\n    print(\"No train.tsv found automatically. Check the Input panel and upload or add dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:30.400072Z","iopub.execute_input":"2025-12-03T15:43:30.400396Z","iopub.status.idle":"2025-12-03T15:43:30.407368Z","shell.execute_reply.started":"2025-12-03T15:43:30.400379Z","shell.execute_reply":"2025-12-03T15:43:30.406484Z"}},"outputs":[{"name":"stdout","text":"Found candidate files:\n  /kaggle/input/fake-news-dataset/train.tsv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell B — check torch / cuda\nimport torch\nprint(\"torch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU name:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:30.408492Z","iopub.execute_input":"2025-12-03T15:43:30.408755Z","iopub.status.idle":"2025-12-03T15:43:36.588205Z","shell.execute_reply.started":"2025-12-03T15:43:30.408731Z","shell.execute_reply":"2025-12-03T15:43:36.587548Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.6.0+cu124\nCUDA available: True\nGPU name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Run this after you enable GPU and restart the kernel\nimport os, torch, subprocess, sys\nprint(\"torch:\", torch.__version__)\nprint(\"CUDA available (torch):\", torch.cuda.is_available())\n\n# If CUDA available, list GPU name and memory\nif torch.cuda.is_available():\n    print(\"GPU name:\", torch.cuda.get_device_name(0))\n    print(\"Device count:\", torch.cuda.device_count())\n    try:\n        # nvidia-smi output (may be available)\n        print(\"\\n--- nvidia-smi ---\")\n        !nvidia-smi\n    except Exception as e:\n        print(\"nvidia-smi not available:\", e)\nelse:\n    print(\"No GPU detected by torch. Make sure you restarted the notebook after enabling accelerator.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:36.589678Z","iopub.execute_input":"2025-12-03T15:43:36.590003Z","iopub.status.idle":"2025-12-03T15:43:36.818319Z","shell.execute_reply.started":"2025-12-03T15:43:36.589984Z","shell.execute_reply":"2025-12-03T15:43:36.817507Z"}},"outputs":[{"name":"stdout","text":"torch: 2.6.0+cu124\nCUDA available (torch): True\nGPU name: Tesla T4\nDevice count: 2\n\n--- nvidia-smi ---\nWed Dec  3 15:43:36 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 1\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport networkx as nx\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# Try to find the train file automatically under /kaggle/input\npossible = [\n    \"/kaggle/input/fake-news-dataset/train.tsv\",\n    \"/kaggle/input/fake-news-dataset/train.csv\",\n    \"/kaggle/input/pulk17-fake-news-dataset/train.tsv\",\n    \"/kaggle/input/train.tsv\"\n]\nINPUT_TSV = None\nfor p in possible:\n    if os.path.exists(p):\n        INPUT_TSV = p\n        break\n\nif INPUT_TSV is None:\n    for root, dirs, files in os.walk(\"/kaggle/input\"):\n        for f in files:\n            if f.lower().endswith((\"train.tsv\",\"train.csv\",\"train.txt\")):\n                INPUT_TSV = os.path.join(root, f)\n                break\n        if INPUT_TSV:\n            break\n\nprint(\"INPUT_TSV =\", INPUT_TSV)\nOUTPUT_DIR = \"/kaggle/working/upfd_synthetic\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(\"OUTPUT_DIR =\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:36.819195Z","iopub.execute_input":"2025-12-03T15:43:36.819422Z","iopub.status.idle":"2025-12-03T15:43:46.876986Z","shell.execute_reply.started":"2025-12-03T15:43:36.819401Z","shell.execute_reply":"2025-12-03T15:43:46.876168Z"}},"outputs":[{"name":"stdout","text":"INPUT_TSV = /kaggle/input/fake-news-dataset/train.tsv\nOUTPUT_DIR = /kaggle/working/upfd_synthetic\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 2\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Memory / graph generation params\nMEAN_USERS = 10   # average synthetic users per article. Reduce to 5 for lower memory\nMIN_USERS = 3\nMAX_USERS = 100\nBA_M = 1          # Barabasi-Albert \"m\" param\n\n# If dataset is large and you only want a subset for testing, set SAMPLE_N to an int (e.g., 5000)\nSAMPLE_N = None   # set to e.g., 5000 to test on a subset\n\n# Embedding model (CPU-friendly)\nEMBED_MODEL = \"distilbert-base-uncased\"\nBATCH_SIZE_EMBED = 32   # lower if memory issues\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nprint(\"If you want GPU speed, enable GPU in Settings -> Accelerator and restart the kernel.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:46.877835Z","iopub.execute_input":"2025-12-03T15:43:46.878374Z","iopub.status.idle":"2025-12-03T15:43:46.890911Z","shell.execute_reply.started":"2025-12-03T15:43:46.878354Z","shell.execute_reply":"2025-12-03T15:43:46.890187Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nIf you want GPU speed, enable GPU in Settings -> Accelerator and restart the kernel.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 3\nif INPUT_TSV is None:\n    raise FileNotFoundError(\"train.tsv not found — upload via 'Add Input' or add dataset to kernel.\")\n\n# Try TSV then CSV\ntry:\n    df = pd.read_csv(INPUT_TSV, sep=\"\\t\", low_memory=False)\nexcept Exception:\n    df = pd.read_csv(INPUT_TSV, sep=\",\", low_memory=False)\n\nprint(\"Loaded rows:\", len(df))\nprint(\"Columns:\", df.columns.tolist())\n\nif SAMPLE_N is not None and SAMPLE_N < len(df):\n    df = df.sample(SAMPLE_N, random_state=SEED).reset_index(drop=True)\n    print(\"Using SAMPLE_N:\", SAMPLE_N, \"-> rows now:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:46.891677Z","iopub.execute_input":"2025-12-03T15:43:46.892073Z","iopub.status.idle":"2025-12-03T15:43:49.114752Z","shell.execute_reply.started":"2025-12-03T15:43:46.892048Z","shell.execute_reply":"2025-12-03T15:43:49.114018Z"}},"outputs":[{"name":"stdout","text":"Loaded rows: 30000\nColumns: ['Unnamed: 0', 'title', 'text', 'subject', 'date', 'label']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\n\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for f in files:\n        print(os.path.join(root, f))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:49.115489Z","iopub.execute_input":"2025-12-03T15:43:49.115763Z","iopub.status.idle":"2025-12-03T15:43:49.122124Z","shell.execute_reply.started":"2025-12-03T15:43:49.115734Z","shell.execute_reply":"2025-12-03T15:43:49.121382Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fake-news-dataset/train.tsv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 1 (simplified & guaranteed to work)\n\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport networkx as nx\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n# MANUALLY SET YOUR PATH HERE (copy from the output of the previous scan)\nINPUT_TSV = \"/kaggle/input/fake-news-dataset/train.tsv\"   # <-- change if needed\n\nprint(\"INPUT_TSV =\", INPUT_TSV)\n\nif not os.path.exists(INPUT_TSV):\n    raise FileNotFoundError(\"The train.tsv file does NOT exist at the given path!\")\n\nOUTPUT_DIR = \"/kaggle/working/upfd_synthetic\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(\"OUTPUT_DIR =\", OUTPUT_DIR)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:49.122854Z","iopub.execute_input":"2025-12-03T15:43:49.123184Z","iopub.status.idle":"2025-12-03T15:43:49.137459Z","shell.execute_reply.started":"2025-12-03T15:43:49.123167Z","shell.execute_reply":"2025-12-03T15:43:49.136896Z"}},"outputs":[{"name":"stdout","text":"INPUT_TSV = /kaggle/input/fake-news-dataset/train.tsv\nOUTPUT_DIR = /kaggle/working/upfd_synthetic\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 2\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Memory / graph generation params\nMEAN_USERS = 10   # lower to reduce memory (set to 5 if needed)\nMIN_USERS = 5\nMAX_USERS = 100\nBA_M = 1\n\n# For quick tests on a small subset set SAMPLE_N = 500 (or None to use all rows)\nSAMPLE_N = 500   # e.g., 500 or 5000 for testing\n\nEMBED_MODEL = \"distilbert-base-uncased\"\nBATCH_SIZE_EMBED = 32\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:49.139303Z","iopub.execute_input":"2025-12-03T15:43:49.139480Z","iopub.status.idle":"2025-12-03T15:43:49.163493Z","shell.execute_reply.started":"2025-12-03T15:43:49.139467Z","shell.execute_reply":"2025-12-03T15:43:49.162970Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 3\nif not os.path.exists(INPUT_TSV):\n    raise FileNotFoundError(\"train.tsv not found at INPUT_TSV path.\")\n\ntry:\n    df = pd.read_csv(INPUT_TSV, sep=\"\\t\", low_memory=False)\nexcept Exception:\n    df = pd.read_csv(INPUT_TSV, sep=\",\", low_memory=False)\n\nprint(\"Loaded rows:\", len(df))\nprint(\"Columns:\", df.columns.tolist())\n\nif SAMPLE_N is not None and SAMPLE_N < len(df):\n    df = df.sample(SAMPLE_N, random_state=SEED).reset_index(drop=True)\n    print(\"Using SAMPLE_N:\", SAMPLE_N, \"-> rows now:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:49.164083Z","iopub.execute_input":"2025-12-03T15:43:49.164325Z","iopub.status.idle":"2025-12-03T15:43:50.279294Z","shell.execute_reply.started":"2025-12-03T15:43:49.164308Z","shell.execute_reply":"2025-12-03T15:43:50.278479Z"}},"outputs":[{"name":"stdout","text":"Loaded rows: 30000\nColumns: ['Unnamed: 0', 'title', 'text', 'subject', 'date', 'label']\nUsing SAMPLE_N: 500 -> rows now: 500\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 4\nif \"text\" in df.columns:\n    text_col = \"text\"\nelif \"title\" in df.columns:\n    text_col = \"title\"\nelse:\n    text_col = df.columns[0]\nprint(\"Using text column:\", text_col)\n\npossible_labels = [c for c in df.columns if c.lower() in (\"label\",\"labels\",\"target\",\"truth\",\"fake\",\"is_fake\")]\nlabel_col = possible_labels[0] if possible_labels else None\nprint(\"Detected label column:\", label_col)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:50.280185Z","iopub.execute_input":"2025-12-03T15:43:50.280528Z","iopub.status.idle":"2025-12-03T15:43:50.291097Z","shell.execute_reply.started":"2025-12-03T15:43:50.280506Z","shell.execute_reply":"2025-12-03T15:43:50.290402Z"}},"outputs":[{"name":"stdout","text":"Using text column: text\nDetected label column: label\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 5\nprint(\"Loading embedding model:\", EMBED_MODEL)\ntokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)\nmodel = AutoModel.from_pretrained(EMBED_MODEL).to(device)\nmodel.eval()\n\ndef embed_texts(text_list, batch_size=BATCH_SIZE_EMBED):\n    embs = []\n    with torch.no_grad():\n        for i in range(0, len(text_list), batch_size):\n            batch = text_list[i:i+batch_size]\n            enc = tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n            enc = {k: v.to(device) for k, v in enc.items()}\n            out = model(**enc, return_dict=True)\n            if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n                pooled = out.pooler_output\n            else:\n                pooled = out.last_hidden_state[:, 0, :]\n            embs.append(pooled.cpu().numpy())\n    embs = np.vstack(embs)\n    return embs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:43:50.291837Z","iopub.execute_input":"2025-12-03T15:43:50.292092Z","iopub.status.idle":"2025-12-03T15:44:33.722859Z","shell.execute_reply.started":"2025-12-03T15:43:50.292076Z","shell.execute_reply":"2025-12-03T15:44:33.722191Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model: distilbert-base-uncased\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b2631bab706484ebd661c2d036b80aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3644348303f74059838a51d6f7f04708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b6219cd168477a8be3e57018302dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc2e2ace32a41ba89e453560603a446"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-03 15:44:01.579116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764776641.966455      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764776642.078842      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa7dd847120243129daa589537259c1e"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Cell 6\ntexts = df[text_col].fillna(\"\").astype(str).tolist()\nprint(\"Computing embeddings for\", len(texts), \"articles. This may take several minutes on CPU.\")\nnews_embs = embed_texts(texts)\nprint(\"news_embs shape:\", news_embs.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:49:25.656132Z","iopub.execute_input":"2025-12-03T15:49:25.656483Z","iopub.status.idle":"2025-12-03T15:49:29.838129Z","shell.execute_reply.started":"2025-12-03T15:49:25.656460Z","shell.execute_reply":"2025-12-03T15:49:29.837451Z"}},"outputs":[{"name":"stdout","text":"Computing embeddings for 500 articles. This may take several minutes on CPU.\nnews_embs shape: (500, 768)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 7\ntotal = len(df)\nglobal_node = 0\nedges = []\nnode_graph_id = []\ngraph_labels = []\nbert_feats = []\nprofile_feats = []\ncontent_feats = []\n\nfor gid in tqdm(range(total), desc=\"building graphs\"):\n    row = df.iloc[gid]\n    # label conversion\n    if label_col:\n        val = row[label_col]\n        if isinstance(val, str):\n            lab = 1 if val.strip().lower() in (\"fake\",\"1\",\"true\",\"yes\") else 0\n        else:\n            try:\n                lab = int(val)\n            except:\n                lab = 1 if float(val) > 0.5 else 0\n    else:\n        lab = int(random.random() < 0.5)\n    graph_labels.append(lab)\n\n    k = np.random.poisson(MEAN_USERS)\n    k = int(min(max(k, MIN_USERS), MAX_USERS))\n\n    if k <= BA_M:\n        G = nx.empty_graph(k)\n    else:\n        G = nx.barabasi_albert_graph(k, BA_M, seed=SEED+gid)\n\n    news_node = global_node\n    global_node += 1\n    bert_feats.append(news_embs[gid].astype(np.float32))\n    profile_feats.append(np.zeros(10, dtype=np.float32))\n    content_feats.append(np.zeros(310, dtype=np.float32))\n    node_graph_id.append(gid)\n\n    local_to_global = {}\n    for li in range(k):\n        uid = global_node\n        local_to_global[li] = uid\n        global_node += 1\n\n        noise = np.random.normal(scale=0.01, size=news_embs.shape[1]).astype(np.float32)\n        user_bert = (news_embs[gid].astype(np.float32) + noise).astype(np.float32)\n        bert_feats.append(user_bert)\n\n        prof = np.random.randn(10).astype(np.float32)\n        prof = prof / (np.linalg.norm(prof)+1e-9)\n        profile_feats.append(prof)\n\n        small_vec = (np.random.randn(300) * 0.01).astype(np.float32)\n        content_feats.append(np.concatenate([prof, small_vec]))\n        node_graph_id.append(gid)\n\n        edges.append((news_node, uid))\n\n    for (u,v) in G.edges():\n        edges.append((local_to_global[u], local_to_global[v]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:49:52.380540Z","iopub.execute_input":"2025-12-03T15:49:52.380852Z","iopub.status.idle":"2025-12-03T15:49:52.836588Z","shell.execute_reply.started":"2025-12-03T15:49:52.380830Z","shell.execute_reply":"2025-12-03T15:49:52.835842Z"}},"outputs":[{"name":"stderr","text":"building graphs: 100%|██████████| 500/500 [00:00<00:00, 1130.21it/s]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 8\nnode_graph_id = np.array(node_graph_id, dtype=np.int32)\ngraph_labels = np.array(graph_labels, dtype=np.int64)\nbert_arr = np.vstack(bert_feats).astype(np.float32)\nprofile_arr = np.vstack(profile_feats).astype(np.float32)\ncontent_arr = np.vstack(content_feats).astype(np.float32)\n\nnp.save(os.path.join(OUTPUT_DIR, \"node_graph_id.npy\"), node_graph_id)\nnp.save(os.path.join(OUTPUT_DIR, \"graph_labels.npy\"), graph_labels)\nnp.savez_compressed(os.path.join(OUTPUT_DIR, \"new_bert_feature.npz\"), data=bert_arr)\nnp.savez_compressed(os.path.join(OUTPUT_DIR, \"new_profile_feature.npz\"), data=profile_arr)\nnp.savez_compressed(os.path.join(OUTPUT_DIR, \"new_content_feature.npz\"), data=content_arr)\n\nwith open(os.path.join(OUTPUT_DIR, \"A.txt\"), \"w\") as f:\n    for u,v in edges:\n        f.write(f\"{u} {v}\\n\")\n\nprint(\"Saved to\", OUTPUT_DIR)\nprint(\"Files:\", os.listdir(OUTPUT_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:51:42.746733Z","iopub.execute_input":"2025-12-03T15:51:42.747252Z","iopub.status.idle":"2025-12-03T15:51:43.956038Z","shell.execute_reply.started":"2025-12-03T15:51:42.747219Z","shell.execute_reply":"2025-12-03T15:51:43.955369Z"}},"outputs":[{"name":"stdout","text":"Saved to /kaggle/working/upfd_synthetic\nFiles: ['A.txt', 'node_graph_id.npy', 'new_content_feature.npz', 'new_profile_feature.npz', 'graph_labels.npy', 'new_bert_feature.npz']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 9\nprint(\"num graphs (articles):\", total)\nprint(\"num nodes:\", node_graph_id.shape[0])\nprint(\"num edges:\", len(edges))\nprint(\"bert shape:\", bert_arr.shape)\nprint(\"profile shape:\", profile_arr.shape)\nprint(\"content shape:\", content_arr.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T15:52:23.631382Z","iopub.execute_input":"2025-12-03T15:52:23.632139Z","iopub.status.idle":"2025-12-03T15:52:23.636742Z","shell.execute_reply.started":"2025-12-03T15:52:23.632112Z","shell.execute_reply":"2025-12-03T15:52:23.635990Z"}},"outputs":[{"name":"stdout","text":"num graphs (articles): 500\nnum nodes: 5425\nnum edges: 9350\nbert shape: (5425, 768)\nprofile shape: (5425, 10)\ncontent shape: (5425, 310)\n","output_type":"stream"}],"execution_count":18}]}